
# graph-analytics-starter-pack

[![Awesome](https://awesome.re/badge.svg)](https://github.com/guaiyoui/graph-analytics-starter-pack) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)


Graph Analyticså’ŒAI4DBç›¸å…³å­¦ä¹ èµ„æ–™/è·¯å¾„

---

## ç›®å½•
- [graph-analytics-starter-pack](#graph-analytics-starter-pack)
  - [ç›®å½•](#ç›®å½•)
  - [å¯¼è®º](#å¯¼è®º)
  - [è§†é¢‘è¯¾ç¨‹](#è§†é¢‘è¯¾ç¨‹)
    - [ä¸»è¦è¯¾ç¨‹](#ä¸»è¦è¯¾ç¨‹)
    - [å‚è€ƒè¯¾ç¨‹](#å‚è€ƒè¯¾ç¨‹)
    - [é‡ç‚¹ç« èŠ‚](#é‡ç‚¹ç« èŠ‚)
  - [1: Cohesive Subgraph Discovery](#1-cohesive-subgraph-discovery)
    - [1.1 Subgraph-model-based Community Search](#11-subgraph-model-based-community-search)
    - [1.2 Metric-based Community Search](#12-metric-based-community-search)
    - [1.3 Learning-based Community Search](#13-learning-based-community-search)
  - [2: Generalized Anomaly Detection](#2-generalized-anomaly-detection)
    - [2.1 Survey of anomaly detection and Benchmarks](#21-survey-of-anomaly-detection-and-benchmarks)
    - [2.2 Anomaly Detection](#22-anomaly-detection)
    - [2.3 Fraud Detection](#23-fraud-detection)
  - [3: AIGC-LLM](#3-aigc-llm)
    - [3.1 Survey of AIGC-LLM](#31-survey-of-aigc-llm)
    - [3.2 Theory of AIGC-LLM](#32-theory-of-aigc-llm)
    - [3.3 Prompt Learning](#33-prompt-learning)
      - [Prompt **Engineering Techniques**](#prompt-engineering-techniques)
      - [In-context Learning](#in-context-learning)
      - [Parameter-Efficient Fine-Tuning (PEFT)](#parameter-efficient-fine-tuning-peft)
      - [Reasoning with Large Language Models](#reasoning-with-large-language-models)
      - [Multimodal Prompt](#multimodal-prompt)
      - [Evaluation \& Reliability](#evaluation--reliability)
      - [Others](#others)
    - [3.4 Foundation Models](#34-foundation-models)
      - [3.4.1 Encoder-only Architecture](#341-encoder-only-architecture)
      - [3.4.2 Decoder-only Architecture](#342-decoder-only-architecture)
      - [3.4.3 Encoder-decoder Architecture](#343-encoder-decoder-architecture)
      - [3.4.4 Other](#344-other)
    - [3.5 Related Repos](#35-related-repos)
    - [3.6 Datasets of LLM-AIGC](#36-datasets-of-llm-aigc)
    - [3.7 Tools for LLM-AIGC](#37-tools-for-llm-aigc)
      - [Open-Source LLMs](#open-source-llms)
      - [Prompt Learning](#prompt-learning)
      - [CoT](#cot)
      - [Development](#development)
      - [ChatBots](#chatbots)
  - [4: Graph Similarity Computation](#4-graph-similarity-computation)
  - [5: Subgraph Matching and Counting](#5-subgraph-matching-and-counting)
  - [6: Cardinality Estimation](#6-cardinality-estimation)
  - [7: Graph for DB and tabular data](#7-graph-for-db-and-tabular-data)
  - [8: Vector Database](#8-vector-database)
  - [9: Differential Privacy](#9-differential-privacy)
  - [10: å…¶ä»–](#9-å…¶ä»–)
    - [è®ºæ–‡å†™ä½œ](#è®ºæ–‡å†™ä½œ)
    - [ç”»å›¾](#ç”»å›¾)
    - [å·¥å…·](#å·¥å…·)
- [graph analytics on GPUs](./GPU/)

## å¯¼è®º
Graph Analyticså’ŒAI for Databaseæ˜¯å½“ä»Šæ•°æ®åˆ†æå’Œäººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸¤ä¸ªé‡è¦åˆ†æ”¯ã€‚Graph Analyticsé€šè¿‡åˆ†æå›¾å½¢æ•°æ®æ¥æ­ç¤ºæ•°æ®èƒŒåçš„æ¨¡å¼å’Œå…³ç³»ï¼Œå¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨æ•°æ®ã€‚AI4DBåˆ™åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯æ¥å¤„ç†å’Œç®¡ç†å¤§è§„æ¨¡æ•°æ®åº“ã€è§£å†³NP-hardçš„å›¾ç›¸å…³é—®é¢˜ï¼Œæé«˜æ•°æ®å¤„ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

åœ¨Graph Analyticså’ŒAI for Databaseé¢†åŸŸï¼Œæˆ‘ä»¬ä¸€èˆ¬å…³æ³¨æ¥è‡ªä»¥ä¸‹ä¼šè®®çš„å­¦æœ¯è®ºæ–‡:

<table>
    <tr>
        <th>Category</th><th>Conference</th><th>Link</th><th>Comment</th>
    </tr>
    <tr>
        <td rowspan="3">Database</td><td>SIGMOD</td><td> <a href="https://dblp.org/db/conf/sigmod/sigmod2022.html" target="_blank">DBLP, </a>  <a href="https://2022.sigmod.org/" target="_blank">å®˜ç½‘</a>  </td> <td> Pioneering conference in Database</td>
    </tr>
    <tr>
        <td>VLDB</td><td><a href="http://vldb.org/pvldb/volumes/16/" target="_blank">VLDB</a></td> <td> </td>
    </tr>
    <tr>
        <td>ICDE</td><td> <a href="https://icde2023.ics.uci.edu/" target="_blank">ICDE</a> </td>  <td> </td>
    </tr>
    <tr>
        <td rowspan="3">AI/ML/DL</td><td>ICML</td><td> <a href="https://dblp.org/db/conf/icml/icml2022.html" target="_blank">ICML</a> </td>  <td> </td>
    </tr>
    <tr>
        <td>ICLR</td><td> <a href="https://dblp.org/db/conf/iclr/iclr2022.html" target="_blank">ICLR</a> </td>  <td> </td>
    </tr>
    <tr>
        <td>NeurIPS</td><td> <a href="https://papers.nips.cc/paper/2022" target="_blank">NeurIPS</a> </td>  <td> </td>
    </tr>
    <tr>
        <td rowspan="1">Data Mining</td><td>KDD</td><td> <a href="https://kdd.org/kdd2022/paperRT.html" target="_blank">KDD</a> </td>  <td> </td>
    </tr>
</table>
GNNçš„å…¨é¢ä»‹ç»[A Comprehensive Survey on Graph Neural Networks](https://ieeexplore.ieee.org/document/9046288)

GNNå¸¸ç”¨pytorchå®ç°ï¼Œè¿™é‡Œæœ‰ä¸€ç³»åˆ—ä½¿ç”¨pytorchçš„æ•™ç¨‹https://pytorch-geometric.readthedocs.io/en/latest/



å¦‚æœå¯¹æ·±åº¦å­¦ä¹ ä¸å¤Ÿäº†è§£ï¼Œå¯ä»¥çœ‹ï¼ˆä»¥ä¸‹ä¸ºå‚è€ƒï¼Œä¸éœ€è¦éƒ½çœ‹ï¼‰

* è¿™æœ¬ä¹¦ï¼šDeep Learninghttps://github.com/exacity/deeplearningbook-chinese
* è¿™é—¨è¯¾ï¼šBilibiliæœç´¢ æå®æ¯… æœºå™¨å­¦ä¹ 
* è¯¾+ä¹¦ï¼šåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ 



å…¶å®ƒå¯èƒ½æœ‰å‚è€ƒä»·å€¼çš„æ–¯å¦ç¦å¤§å­¦è¯¾ç¨‹ï¼šcs224n(NLP) cs224w(graph) cs229(ML) cs231n(CV) cs285(RL)




## è§†é¢‘è¯¾ç¨‹
### ä¸»è¦è¯¾ç¨‹
- [Stanford CS224W Machine Learning with Graphs è¯¾ç¨‹ç½‘å€](http://web.stanford.edu/class/cs224w/)
- [Stanford CS224W Machine Learning with Graphs è¯¾ç¨‹è§†é¢‘](https://www.bilibili.com/video/BV1RZ4y1c7Co/?spm_id_from=333.337.search-card.all.click&vd_source=eb83fc5d65c5d8ce4504000a8b1a7056)
- [UNSW COMP9312 Data Analytics for Graphs](https://github.com/guaiyoui/awesome-graph-analytics/tree/main/COMP9312)

### å‚è€ƒè¯¾ç¨‹
- [Stanford CS520 Knowledge Graphs (2021)](https://www.bilibili.com/video/BV1hb4y1r7fF/?from=search&seid=6234955209527085652&spm_id_from=333.337.0.0&vd_source=eb83fc5d65c5d8ce4504000a8b1a7056)
- [Stanford CS246 å¤§æ•°æ®æŒ–æ˜ (2019)](https://www.bilibili.com/video/BV1SC4y187x1/?from=search&seid=1692751967493851255&spm_id_from=333.337.0.0&vd_source=eb83fc5d65c5d8ce4504000a8b1a7056)
- [Stanford Course Explore](https://explorecourses.stanford.edu/search?view=catalog&academicYear=&page=0&q=CS&filter-departmentcode-CS=on&filter-coursestatus-Active=on&filter-term-Autumn=on)

### é‡ç‚¹ç« èŠ‚
| Week  | Content  | Reading List  | Material  |
|---|---|---|---|
|1| Node Embedding  | [1: DeepWalk: Online Learning of Social Representations](https://arxiv.org/pdf/1403.6652.pdf)<br>[2: node2vec: Scalable Feature Learning for Networks](https://arxiv.org/pdf/1607.00653.pdf) | èŠ‚ç‚¹è¡¨å¾æ˜¯learningä¸­æœ€åŸºç¡€çš„ä¸€ä¸ªé—®é¢˜ã€‚å¯ä»¥å‚è€ƒCS224W 3rdå’ŒCOMP9312 week6çš„å†…å®¹ã€‚ä¼ ç»Ÿçš„çŸ©é˜µåˆ†è§£æ–¹æ³•å¯ä»¥å‚è€ƒï¼š[çŸ©é˜µåˆ†è§£çš„pythonå®ç°](https://blog.csdn.net/qq_43741312/article/details/97548944) | 
|2| Graph Neural Networks  | [1: Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf)<br>[2: Graph Attention Networks](https://arxiv.org/pdf/1710.10903.pdf)   | æ¨¡å‹çš„ç»“æ„æ˜¯learningçš„æ ¸å¿ƒ(CS224W 4th)ï¼Œreading listç»™äº†ä¸¤ä¸ªç»å…¸æ¨¡å‹: GCNå’ŒGATã€‚å…³äºç¥ç»ç½‘ç»œä¸­æ¯ä¸€å±‚çš„ç»“æ„ï¼Œå¯ä»¥å‚è€ƒtutorial: [Learning basic](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/tutorials/Tutorial5_inclasscode.ipynb) | 
|3| GNN Augmentation and Training   |  [1: RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/pdf/1902.10197.pdf)<br>[2: Hyper-Path-Based Representation Learning for Hyper-Networks.](https://arxiv.org/abs/1908.09152) | ä»‹ç»æ•´ä¸ªGNNçš„æµç¨‹ï¼Œå¯ä»¥å‚è€ƒCS224W 6thã€‚reading listç»™äº†å…¶ä»–representation learningçš„æ–¹æ³•å’Œåœ¨knowledge graph/hypergraphä¸Šçš„æ‹“å±•ã€‚æ€ä¹ˆembeddingå¯ä»¥å‚è€ƒtutorial: [Node embedding](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/tutorials/tutorial6_Node_Embedding.ipynb)  |
|4| Theory of Graph Neural Networks  | [1: Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning](https://arxiv.org/abs/2103.00113)<br>[2: Sub-graph Contrast for Scalable Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2009.10273)  | å¯¹ç¥ç»ç½‘ç»œçš„åˆ†æï¼Œå¯ä»¥å‚è€ƒCS224w 7th. reading listç»™äº†ç›®å‰ä¸»æµçš„self supervisedçš„embeddingæ–¹æ³•å’Œç½‘ç»œæ¡†æ¶ã€‚æ€ä¹ˆç”¨learningæ¥åšåŸºç¡€ä»»åŠ¡ï¼Œå‚è€ƒtutorial: [Downstream Application: node classification, link prediction and graph classification](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/tutorials/Tutorial7_Downstream_Applications_Template.ipynb)  |
|5| Label Propagation on Graphs |  [1: GLSearch: Maximum Common Subgraph Detection via Learning to Search](http://proceedings.mlr.press/v139/bai21e/bai21e.pdf)<br>[2: Computing Graph Edit Distance via Neural Graph Matching](https://www.vldb.org/pvldb/vol16/p1817-cheng.pdf) | å‚è€ƒCS224W 8th. reading listç»™äº†graph similarity computation (NP hard)é—®é¢˜çš„ä¸¤ç§è§£å†³æ–¹æ³•ã€‚è‡ªå·±å®ç°GCNçš„ç»“æ„ï¼Œå¯ä»¥å‚è€ƒtutorial: [Graph Convolutional Network](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/tutorials/tutorial_8.ipynb)  |
|6| Subgraph Matching and Counting |  [1: A Learned Sketch for Subgraph Counting](https://dl.acm.org/doi/pdf/10.1145/3448016.3457289)<br>[2: Neural Subgraph Counting with Wasserstein Estimator](https://dl.acm.org/doi/pdf/10.1145/3514221.3526163) | å‚è€ƒCS224W 12nd. reading listç»™äº†subgraph counting (NP hard)é—®é¢˜çš„ä¸¤ç¯‡æ–‡ç« (from sigmod 2021 and sigmod 2022)ã€‚subconè®ºæ–‡çš„ä»“åº“ä»£ç åœ¨ [Contrastive Learning on Graph](https://github.com/yzjiao/Subg-Con)  |



<p id="CohesiveSubgraphDiscovery"></p>

## 1: Cohesive Subgraph Discovery
Cohesive Subgraph Discoveryæ˜¯ä¸€ç§åœ¨å›¾å½¢æ•°æ®ä¸­å¯»æ‰¾å…·æœ‰é«˜åº¦å†…èšæ€§çš„å­å›¾çš„é—®é¢˜ã€‚è¿™ç¯‡surveyï¼š[A Survey on Machine Learning Solutions for Graph Pattern Extraction](https://arxiv.org/abs/2204.01057)ï¼ˆpay close attention to Ch2.6 **community search**ï¼‰å¾ˆå¥½çš„è®²æ¸…æ¥šäº†æˆ‘ä»¬å·¥ä½œçš„å‡ ç¯‡baselineæ–‡ç« ã€‚

### 1.1 Subgraph-model-based Community Search
Subgraph-model-based community search model the community as various subgraph models, e.g., k-core, k-truss, and k connected component.

### 1.2 Metric-based Community Search
The metric-based community search methods aims to find a connected subgraph that contains the query nodes and has the largest metric, e.g., density, modularity....

| Conference | Paper  |  Material | Abstract | Highlights |
|---|---|---|---|---|
|SIGMOD2022|[DMCS: Density Modularity based Community Search](https://dl.acm.org/doi/abs/10.1145/3514221.3526137)|---| maximize the density modularity| Propose a new modulariity called density modularity to alleviate free-rider effect and resolution limit problem. |
|VLDB2015|[Robust local community detection: on free rider effect and its elimination](http://www.vldb.org/pvldb/vol8/p798-wu.pdf)|---| maximize the query biased density | Systematically study the many goodness functions, and provide detailed proof. |




### 1.3 Learning-based Community Search
Learning-based community searchçš„æ–¹æ³•ï¼Œä¸€èˆ¬æŠŠé—®é¢˜modelæˆnode classificationçš„ä»»åŠ¡ã€‚

| Conference | Paper  |  Material | Abstract | Highlights |
|---|---|---|---|---|
|VLDB2021|[ICS-GNN: lightweight interactive community search via graph neural network](https://dl.acm.org/doi/pdf/10.14778/3447689.3447704)|[[code]](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/files/ics-gnn.zip)| Find a community in multiple iterations (i.e., hops)|---|
|VLDB2022|[Query Driven-Graph Neural Networks for Community Search](https://arxiv.org/abs/2104.03583)|[[code]](https://github.com/lizJyl/Codes-for-Peer-Review-of-VLDB-August-337)|QD-GNN and AQD-GNN for community search and attributed community search respectively|Take query into account. Study multiple CS-related settings.|
|ICDE2023|[Community Search: A Meta-Learning Approach](https://arxiv.org/abs/2201.00288)|---|CS using small data|---|
|ICDE2023|[COCLEP: Contrastive Learning-based Semi-Supervised Community Search](https://siqiangluo.com/docs/COCLEP__Contrastive_Learning_based_Semi_Supervised_Community_Search__camera_ready_.pdf)|[[code]](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/files/COCLEP.zip)|è®ºæ–‡é’ˆå¯¹ç›®å‰åŸºäºæ·±åº¦å­¦ä¹ çš„ç¤¾åŒºæœç´¢æ¨¡å‹ä¾èµ–å¤§é‡å®é™…éš¾ä»¥è·å–çš„æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ å’Œæ•°æ®åˆ†åŒºçš„ç¤¾åŒºæœç´¢æ–¹æ³•(COCLEP)ï¼Œåªéœ€æå°‘é‡çš„æ ‡ç­¾å³å¯å®ç°é«˜æ•ˆä¸”æœ‰æ•ˆçš„ç¤¾åŒºæŸ¥è¯¢ä»»åŠ¡ï¼Œå…¶åŸºæœ¬åŸç†æ˜¯é€šè¿‡æ‰€æå‡ºçš„å›¾ç¥ç»ç½‘ç»œå’Œæ ‡ç­¾æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å™¨æ¥å­¦ä¹ æŸ¥è¯¢ä¾èµ–çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä»ç†è®ºä¸Šè¯æ˜äº†å¯ä»¥åˆ©ç”¨æœ€å°å‰²å°†COCLEPæ‰©å±•ç”¨äºå¤§å‹æ•°æ®é›†ã€‚|---|

<p id="GeneralizedAnomalyDetection"></p>

## 2: Generalized Anomaly Detection
Generalized Anomaly DetectionåŒ…æ‹¬äº†å¾ˆå¤šç±»ä¼¼çš„é—®é¢˜ï¼Œæ¯”å¦‚: anomaly detection, novelty detection, open set recognition, out-of-distribution detection å’Œ outlier detection.

### 2.1 Survey of anomaly detection and Benchmarks
- Generalized Out-of-Distribution Detection: A Survey [[paper]](https://arxiv.org/pdf/2110.11334.pdf)

- DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection [[paper]](https://arxiv.org/abs/2207.03579) [[project page]](https://dgraph.xinye.com/dataset)
- ADBench: Anomaly Detection Benchmark [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/cf93972b116ca5268827d575f2cc226b-Abstract-Datasets_and_Benchmarks.html) [[project page]](https://github.com/Minqi824/ADBench/)

### 2.2 Anomaly Detection

| Conference | Paper  |  Material | Abstract | Highlights |
|---|---|---|---|---|
|ICLR2022|[Anomaly detection for tabular data with internal contrastive learning.](https://openreview.net/forum?id=_hszZbt46bT)|[[code]](https://openreview.net/forum?id=_hszZbt46bT)| KNN and Contrastive Learning for Tabular data| -- |
|ICDE2023|[Unsupervised Graph Outlier Detection: Problem Revisit, New Insight, and Superior Method.](https://fanzhangcs.github.io/)|---| --- | --- |


### 2.3 Fraud Detection
- [å…¥é—¨ç»¼è¿°è®ºæ–‡](https://github.com/safe-graph/graph-fraud-detection-papers#survey-paper-back-to-top)
- [å…¥é—¨è®ºæ–‡åˆ—è¡¨](https://github.com/safe-graph/graph-fraud-detection-papers)
- [å…¥é—¨ä»£ç demo](https://github.com/finint/antifraud)
- [TKDE Community Awareåæ´—é’± Anti-Money Laundering by Group-Aware Deep Graph Learning](https://ieeexplore.ieee.org/document/10114503)
- [AAAI Risk-Awareåè¯ˆéª— Semi-Supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation](https://arxiv.org/pdf/2003.01171.pdf)
- [TKDE Spatial-Awareåè¯ˆéª— Graph Neural Network for Fraud Detection via Spatial-temporal Attention](https://ieeexplore.ieee.org/abstract/document/9204584)


<p id="AIGCLLM"></p>

## 3: AIGC-LLM

### 3.1 Survey of AIGC-LLM

- [**Augmented Language Models: a Survey](https://doi.org/10.48550/arXiv.2302.07842), Arxiv, 2023.02.15**
- [**A Survey for In-context Learning](https://doi.org/10.48550/arXiv.2301.00234), Arxiv, 2022.12.31**
- [**Reasoning with Language Model Prompting: A Survey](https://doi.org/10.48550/arXiv.2212.09597), Arxiv, 2022.12.19**
- [**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://doi.org/10.1145/3560815), Arxiv, 2021.07.28**
- [**Emergent Abilities of Large Language Models](https://doi.org/10.48550/arXiv.2206.07682), Arxiv, 2022.06.15**
- [**Towards Reasoning in Large Language Models: A Survey](https://doi.org/10.48550/arXiv.2212.10403), Arxiv, 2022.12.20**

### 3.2 Theory of AIGC-LLM

- **[A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks](https://arxiv.org/abs/2010.03648), 2020.10.7**
- **[Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning](https://arxiv.org/abs/2106.09226), 2021.6.17**

### 3.3 Prompt Learning

#### Prompt **Engineering Techniques**

- **[Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://doi.org/10.48550/arXiv.2302.12822)**Â ï¼ˆ**2023.02.24**ï¼‰
- **[Guiding Large Language Models via Directional Stimulus Prompting](https://doi.org/10.48550/arXiv.2302.11520)**Â ï¼ˆ**2023.02.22**ï¼‰
- **[Progressive Prompts: Continual Learning for Language Models](https://doi.org/10.48550/arXiv.2301.12314), 2023.01.29**
- **[Batch Prompting: Efficient Inference with Large Language Model APIs](https://doi.org/10.48550/arXiv.2301.08721)**Â ï¼ˆ**2023.01.19**ï¼‰
- **[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://doi.org/10.48550/arXiv.2212.09741)**Â ï¼ˆ**2022.12.19**ï¼‰
- **[Successive Prompting for Decomposing Complex Questions](https://doi.org/10.48550/arXiv.2212.04092)**Â ï¼ˆ**2022.12.08**ï¼‰
- **[Promptagator: Few-shot Dense Retrieval From 8 Examples](https://doi.org/10.48550/arXiv.2209.11755)**Â ï¼ˆ**2022.09.23**ï¼‰
- **[Black-box Prompt Learning for Pre-trained Language Models](https://arxiv.org/abs/2201.08531)**Â ï¼ˆ**2022.01.21**ï¼‰
- **[Design Guidelines for Prompt Engineering Text-to-Image Generative Models](https://doi.org/10.1145/3491102.3501825)**Â ï¼ˆ**2021.09.14**ï¼‰
- **[Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://doi.org/10.1145/3411763.3451760)**Â ï¼ˆ**2021.02.15**ï¼‰
- [**Making Pre-trained Language Models Better Few-shot Learners](https://doi.org/10.18653/v1/2021.acl-long.295), ACL, 2021.01.01**
- [**Eliciting Knowledge from Language Models Using Automatically Generated Prompts](https://doi.org/10.18653/v1/2020.emnlp-main.346), EMNLP, 2020.10.29**
- **[Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification](https://doi.org/10.5282/UBM/EPUB.74034)**Â ï¼ˆ**2020.10.26**ï¼‰

#### In-context Learning

- **[Larger language models do in-context learning differently](https://doi.org/10.48550/arXiv.2303.03846)**Â ï¼ˆ**2023.03.07**ï¼‰
- **[Language Model Crossover: Variation through Few-Shot Prompting](https://doi.org/10.48550/arXiv.2302.12170)**Â ï¼ˆ**2023.02.23**ï¼‰
- **[How Does In-Context Learning Help Prompt Tuning?](https://doi.org/10.48550/arXiv.2302.11521)**Â ï¼ˆ**2023.02.22**ï¼‰
- **[Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning](https://doi.org/10.48550/arXiv.2301.11916)**Â ï¼ˆ**2023.01.27**ï¼‰
- **[Transformers as Algorithms: Generalization and Stability in In-context Learning](https://arxiv.org/abs/2301.07067)**Â ï¼ˆ**2023.01.17**ï¼‰
- **[OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://doi.org/10.48550/arXiv.2212.12017), Meta,**ï¼ˆ**2022.12.22**ï¼‰
- [**Finetuned Language Models are Zero-Shot Learners](https://arxiv.org/abs/2109.01652),** ICLR, ï¼ˆ2021.9.3ï¼‰
    
    FLANï¼Œå¤šä»»åŠ¡ instruction tuning
    
- [**Learning To Retrieve Prompts for In-Context Learning](https://arxiv.org/abs/2112.08633),** NAACL, ï¼ˆ2022.12.16ï¼‰
    
    Prompt é€‰æ‹©å¯¹æ¨¡å‹æ•ˆæœæœ‰å½±å“ï¼Œä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨ç›¸ä¼¼åº¦çš„æ–¹å¼æŒ‘é€‰åˆé€‚çš„æ ·æœ¬ï¼Œæœ¬æ–‡æå‡ºç”¨ä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹å¯¹è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªæ ·ä¾‹è¿›è¡Œè¯„åˆ†ï¼Œé€‰å–æœ€åˆé€‚çš„ä½œä¸º demonstration
    

#### Parameter-Efficient Fine-Tuning (PEFT)

- [**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS**](https://arxiv.org/pdf/2106.09685.pdf)
    
    **LoRA** 
    
- [**Prefix-Tuning: Optimizing Continuous Prompts for Generation**](https://aclanthology.org/2021.acl-long.353/)
    
    **Prefix Tuning** 
    
- [**GPT Understands, Too**](https://arxiv.org/pdf/2103.10385.pdf)
    
    **P-Tuning V1** 
    
- [**P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**](https://arxiv.org/pdf/2110.07602.pdf)
    
    **P-Tuning V2** 
    
- [**The Power of Scale for Parameter-Efficient Prompt Tuning**](https://arxiv.org/pdf/2104.08691.pdf)
    
    **Prompt Tuning** 
    

#### Reasoning with Large Language Models

- [**Automatic Chain of Thought Prompting in Large Language Models**](https://arxiv.org/abs/2210.03493)
    
    Auto-CoT
    
- **[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)**
    
    Manual-CoT
    
- **[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916), NeurIPS, 2022**
    
    Zero-CoT
    

#### Multimodal Prompt

- [**Multimodal Chain-of-Thought Reasoning in Language Models**](https://arxiv.org/pdf/2302.00923.pdf)

#### Evaluation & Reliability

#### Others

- RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation, VLDB, 2021
- Can Foundation Models Wrangle Your Data?, VLDB, 2023

### 3.4 Foundation Models


#### 3.4.1 Encoder-only Architecture

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018.10.11)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) ï¼ˆ2019.09.26ï¼‰
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) ï¼ˆ2019.07.26ï¼‰
- [ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2112.12731)  (2021.12.23)

#### 3.4.2 Decoder-only Architecture

- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) ï¼ˆ2023.03.15ï¼‰
- GPT-3 [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) ï¼ˆ2020.05.28ï¼‰
- [JURASSIC-1: TECHNICAL DETAILS AND EVALUATION](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) (2021.08)
- Gopher [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) ï¼ˆ2021.12.08ï¼‰
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239) ï¼ˆ2022.01.20ï¼‰
- Chinchilla [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556) ï¼ˆ2022.03.29ï¼‰
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311.pdf) ï¼ˆ2022.04.05ï¼‰
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) (2022.11.09)
- [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017) (2022.12.22)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) ï¼ˆ2023.02.27ï¼‰

#### 3.4.3 Encoder-decoder Architecture

- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) ï¼ˆ2019.10.29ï¼‰
- T5 [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) ï¼ˆ2019.10.23ï¼‰
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) ï¼ˆ2021.01.11ï¼‰

#### 3.4.4 Other
- [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)  (2022.10.05)


### 3.5 Related Repos

- [Awesome-LLM: a curated list of Large Language Model](https://github.com/Hannibal046/Awesome-LLM)  
- [Awesome resources for in-context learning and prompt engineering: Mastery of the LLMs such as ChatGPT, GPT-3, and FlanT5, with up-to-date and cutting-edge updates](https://github.com/EgoAlpha/prompt-in-context-learning) 
- [This repository contains a hand-curated resources for Prompt Engineering with a focus on Generative Pre-trained Transformer (GPT), ChatGPT, PaLM etc](https://github.com/promptslab/Awesome-Prompt-Engineering) 
- [A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models"](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) 
- [Collection of papers and resources on Reasoning in Large Language Models, including Chain-of-Thought, Instruction-Tuning, and others.](https://github.com/atfortes/LLM-Reasoning-Papers) 


### 3.6 Datasets of LLM-AIGC

- [Alpaca dataset from Stanford, cleaned and curated](https://github.com/gururise/AlpacaDataCleaned)  
- [Alpaca is a dataset of 52,000 instructions and demonstrations generated by OpenAI'sÂ `text-davinci-003`Â engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better.](https://huggingface.co/datasets/tatsu-lab/alpaca/tree/main/data) 


### 3.7 Tools for LLM-AIGC

- [Awesome-LLM: a curated list of Large Language Model](https://github.com/Hannibal046/Awesome-LLM)  
- [Awesome resources for in-context learning and prompt engineering: Mastery of the LLMs such as ChatGPT, GPT-3, and FlanT5, with up-to-date and cutting-edge updates](https://github.com/EgoAlpha/prompt-in-context-learning) 
- [This repository contains a hand-curated resources for Prompt Engineering with a focus on Generative Pre-trained Transformer (GPT), ChatGPT, PaLM etc](https://github.com/promptslab/Awesome-Prompt-Engineering) 
- [A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models"](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) 
- [Collection of papers and resources on Reasoning in Large Language Models, including Chain-of-Thought, Instruction-Tuning, and others.](https://github.com/atfortes/LLM-Reasoning-Papers) 


#### Open-Source LLMs

- [Inference code for LLaMA models](https://github.com/facebookresearch/llama)
- [Code and documentation to train Stanford's Alpaca models, and generate the data.](https://github.com/tatsu-lab/stanford_alpaca)
- [Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)
- [Locally run an Instruction-Tuned Chat-Style LLM](https://github.com/antimatter15/alpaca.cpp)
- [Instruct-tune LLaMA on consumer hardware](https://github.com/tloen/alpaca-lora)
- [éª†é©¼:A Chinese finetuned instruction LLaMA](https://github.com/LC1332/Chinese-alpaca-lora)
- [Alpaca-LoRA as Chatbot service](https://github.com/deep-diver/Alpaca-LoRA-Serve)
- his fine-tunes theÂ [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) model on theÂ [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset using a Databricks notebook [The repo](https://github.com/databrickslabs/dolly)
- [ChatGLM-6Bï¼šå¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ | An Open Bilingual Dialogue Language Model](https://github.com/THUDM/ChatGLM-6B)
- GPT-J 6B is a transformer model trained using Ben Wang'sÂ **[Mesh Transformer JAX](https://github.com/kingoflolz/mesh-transformer-jax/)**. "GPT-J" refers to the class of model, while "6B" represents the number of trainable parameters. [EleutherAI/gpt-j-6B Â· Hugging Face](https://huggingface.co/EleutherAI/gpt-j-6B)
    
- [ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ, åŸºäºChatGLM-6B + LoRA](https://github.com/mymusise/ChatGLM-Tuning)
- [Open Academic Research on Improving LLaMA to SOTA LLM](https://github.com/AetherCortex/Llama-X)

#### Prompt Learning

- [An Open-Source Framework for Prompt-Learning](https://github.com/thunlp/OpenPrompt)
- [PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft)

#### CoT

- [Benchmarking LLM reasoning performance w. chain-of-thought prompting](https://github.com/FranxYao/chain-of-thought-hub)

#### Development

- [Examples and guides for using the OpenAI API](https://github.com/openai/openai-cookbook)
- [A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.](https://github.com/oobabooga/text-generation-webui)
- [GUI for ChatGPT API](https://github.com/GaiZhenbiao/ChuanhuChatGPT)
- [LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.](https://github.com/jerryjliu/llama_index)
- [The ChatGPT Retrieval Plugin lets you easily search and find personal or work documents by asking questions in everyday language.](https://github.com/openai/chatgpt-retrieval-plugin)
- [Building applications with LLMs through composability](https://github.com/hwchase17/langchain)
- [Containers for machine learning](https://github.com/replicate/cog)
    
    

#### ChatBots

- [An open-source ChatGPT UI.](https://github.com/mckaywrigley/chatbot-ui) 
- [Create UIs for your machine learning model in Python in 3 minutes](https://github.com/gradio-app/gradio)
- [A web interface for chatting with Alpaca through llama.cpp. Fully dockerized, with an easy to use API.](https://github.com/nsarrazin/serge)
- [ChatLLaMAÂ ğŸ“¢Â Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT](https://github.com/juncongmoo/chatllama)
- [Locally running, hands-free ChatGPT](https://github.com/yakGPT/yakGPT)
- [An editor made for programming with AI](https://github.com/getcursor/cursor)
- [ChatGPT å­¦æœ¯ä¼˜åŒ–](https://github.com/binary-husky/chatgpt_academic)
- [myGPTReader is a slack bot that can read any webpage, ebook, video(YouTube) or document and summarize it with chatGPT. It can also talk to you via voice using the content in the channel.](https://github.com/madawei2699/myGPTReader)
- [Use ChatGPT to summarize the arXiv papers.](https://github.com/kaixindelele/ChatPaper)
- [åŸºäº ChatGPT API çš„åˆ’è¯ç¿»è¯‘æµè§ˆå™¨æ’ä»¶å’Œè·¨å¹³å°æ¡Œé¢ç«¯åº”ç”¨ - Browser extension and cross-platform desktop application for translation based on ChatGPT API.](https://github.com/yetone/openai-translator)
- [LLM Chain for answering questions from documents with citations](https://github.com/whitead/paper-qa)
- [Grounded search engine (i.e. with source reference) based on LLM / ChatGPT / OpenAI API. It supports web search, file content search etc.](https://github.com/michaelthwan/searchGPT)
- [An open-source LLM based research assistant that allows you to have a conversation with a research paper](https://github.com/mukulpatnaik/researchgpt)
- [A simple command-line interface tool that allows you to interact with ChatGPT from OpenAI.](https://github.com/p208p2002/heygpt)

<p id="GraphSimilarityComputation"></p>

## 4: Graph Similarity Computation

<p id="SubgraphMatching"></p>

## 5: Subgraph Matching and Counting

<p id="CardinalityEstimation"></p>

## 6: Cardinality Estimation

<p id="Graph4DB"></p>

## 7: Graph for DB and tabular data

Graphs are a valuable tool for representing connections between entities, while tabular or relational data is a convenient and user-friendly way to store information. Researchers frequently employ graphs to depict interdependencies among records, attributes, elements, and schemas within and across tables. It is worth noting that in contemporary usage, the term "tabular deep learning" is often used to refer to the application of deep learning techniques to relational data organized as records, while the term "database" is often reserved to refer specifically to the software and infrastructure used to manage and manipulate such data.


| Conference | Paper  |  Material | Abstract | Highlights |
|---|---|---|---|---|
|PODS2023|[Databases as Graphs: Predictive Queries for Declarative Machine Learning.](https://dl.acm.org/doi/abs/10.1145/3584372.3589939)|---| Using hypergraph to model the relationship behind the records| -- |
|---|[Enabling tabular deep learning when d ge n with an auxiliary knowledge graph](https://arxiv.org/abs/2306.04766)|---| Capture the relation between two attributes by KG | -- |
|CIKM22|[Local Contrastive Feature learning for Tabular Data](https://dl.acm.org/doi/10.1145/3511808.3557630)|---| Capture the relation between two attributes by maximum spanning tree | -- |
|NIPS22|[Learning enhanced representations for tabular data via neighborhood propagation](https://arxiv.org/abs/2206.06587)|---| --- | -- |
|dlpkdd2021|[TabGNN: Multiplex Graph Neural Network for Tabular Data Prediction](https://arxiv.org/abs/2108.09127)|---| --- | -- |






<p id="VectorDB"></p>

## 8: Vector Database

Similarity search at a very large scale.

| Conference | Paper  |  Material | Abstract | Highlights |
|---|---|---|---|---|
|SIGMOD2023|[Near-Duplicate Sequence Search at Scale for Large Language Model Memorization.](https://dl.acm.org/doi/abs/10.1145/3589324)|---| ---| -- |

Talk 1: [Vector Database for Large Language Models in Production (Sam Partee)](https://www.youtube.com/watch?v=9VgpXcfJYvw)

## 9: Differential Privacy
DP & Graph:

DP & ML:
https://github.com/JeffffffFu/Awesome-Differential-Privacy-and-Meachine-Learning


## 10: å…¶ä»–
- [PKU Lanco Lab å…¥é—¨æŒ‡å¯¼](https://github.com/guaiyoui/awesome-graph-analytics/blob/main/files/NLP%E5%85%A5%E9%97%A8%E6%8C%87%E5%AF%BC.pdf)
- [CS224wå­¦ä¹ èµ„æ–™](https://yasoz.github.io/cs224w-zh/#/Introduction-and-Graph-Structure)

### è®ºæ–‡å†™ä½œ
- [Writing tips from MLNLP](https://github.com/MLNLP-World/Paper-Writing-Tips#%E7%99%BE%E5%AE%B6%E4%B9%8B%E8%A8%80)
- [åœ¨çº¿latexç¼–è¾‘å™¨](https://www.latexlive.com/)
### ç”»å›¾
- [https://github.com/guanyingc/python_plot_utils](https://github.com/guanyingc/python_plot_utils)
### å·¥å…·
- [è°·æ­Œå­¦æœ¯](https://scholar.google.com.hk/)
- [ChatGPT](https://chat.openai.com/), [POE: é›†æˆå¤šä¸ªè¯­è¨€æ¨¡å‹](https://poe.com/ChatGPT)
- [ChatGPTå­¦æœ¯æ¶¦è‰²çš„promptå‚è€ƒ](https://github.com/ashawkey/chatgpt_please_improve_my_paper_writing)
